from typing import List, Dict
import google.generativeai as genai
import anthropic
from openai import OpenAI
from xai_sdk import Client as XAIClient
from xai_sdk.chat import user as xai_user, system as xai_system



def call_llm(

    provider: str,

    model: str,

    system_prompt: str,

    user_messages: List[Dict[str, str]],

    max_tokens: int = 2048,

    temperature: float = 0.4,

) -> str:

    """

    user_messages: list of {"role": "user"/"assistant"/"system", "content": "..."}

    """



    if provider == "OpenAI":

        api_key = st.session_state.get("openai_api_key") or os.getenv("OPENAI_API_KEY")

        if not api_key:

            st.error("OpenAI API key is required.")

            return ""

        client = OpenAI(api_key=api_key)

        messages = [{"role": "system", "content": system_prompt}] + user_messages

        resp = client.chat.completions.create(

            model=model,

            messages=messages,

            max_tokens=max_tokens,

            temperature=temperature,

        )

        return resp.choices[0].message.content



    elif provider == "Gemini":

        api_key = st.session_state.get("gemini_api_key") or os.getenv("GEMINI_API_KEY")

        if not api_key:

            st.error("Gemini API key is required.")

            return ""

        genai.configure(api_key=api_key)

        model_obj = genai.GenerativeModel(model)

        full_prompt = f"{system_prompt}\n\n" + "\n\n".join(

            f"{m['role'].upper()}: {m['content']}" for m in user_messages

        )

        resp = model_obj.generate_content(

            full_prompt,

            generation_config=genai.types.GenerationConfig(

                max_output_tokens=max_tokens,

                temperature=temperature,

            ),

        )

        return resp.text



    elif provider == "Anthropic":

        api_key = st.session_state.get("anthropic_api_key") or os.getenv("ANTHROPIC_API_KEY")

        if not api_key:

            st.error("Anthropic API key is required.")

            return ""

        client = anthropic.Anthropic(api_key=api_key)

        # Separate system vs messages explicitly for Anthropic

        messages = [m for m in user_messages if m["role"] != "system"]

        resp = client.messages.create(

            model=model,

            max_tokens=max_tokens,

            temperature=temperature,

            system=system_prompt,

            messages=[{"role": m["role"], "content": m["content"]} for m in messages],

        )

        return "".join(block.text for block in resp.content if hasattr(block, "text"))



    elif provider == "XAI (Grok)":

        # SAMPLE XAI CODE (per your example)

        api_key = st.session_state.get("xai_api_key") or os.getenv("XAI_API_KEY")

        if not api_key:

            st.error("XAI API key is required.")

            return ""

        client = XAIClient(api_key=api_key, timeout=3600)

        chat = client.chat.create(model=model)

        chat.append(xai_system(system_prompt))

        for m in user_messages:

            if m["role"] == "user":

                chat.append(xai_user(m["content"]))

            # For simplicity we ignore assistant history here, or you can add it as needed.



        response = chat.sample()

        return response.content



    else:

        st.error("Unsupported provider.")

        return ""

```



---



## 5. File Handling & New ‚ÄúFile Transform & Deep Summary‚Äù Agent



### 5.1 File Extraction Helpers



```python

from io import BytesIO

import docx2txt

from PyPDF2 import PdfReader



def extract_text_from_pdf(file_bytes: BytesIO) -> str:

    reader = PdfReader(file_bytes)

    texts = []

    for page in reader.pages:

        texts.append(page.extract_text() or "")

    return "\n".join(texts)



def extract_text_from_docx(file_bytes: BytesIO) -> str:

    return docx2txt.process(file_bytes)



def extract_text_from_txt(file_bytes: BytesIO) -> str:

    return file_bytes.read().decode("utf-8", errors="ignore")



def extract_text(file) -> str:

    name = file.name.lower()

    data = BytesIO(file.read())  # copy into memory

    if name.endswith(".pdf"):

        return extract_text_from_pdf(data)

    elif name.endswith(".docx"):

        return extract_text_from_docx(data)

    elif name.endswith(".txt"):

        return extract_text_from_txt(data)

    else:

        st.error("Unsupported format. Please upload PDF, DOCX, or TXT.")

        return ""

```



### 5.2 Markdown vs PDF Transformation



We‚Äôll treat ‚ÄúMarkdown‚Äù as LLM-converted structured text; ‚ÄúPDF‚Äù as a downloadable PDF built from that markdown.



For PDF we can use a very simple markdown-to-text export plus `reportlab` or `fpdf`:



```python

from fpdf import FPDF



def markdown_to_pdf_bytes(md_text: str) -> bytes:

    pdf = FPDF()

    pdf.set_auto_page_break(auto=True, margin=15)

    pdf.add_page()

    pdf.set_font("Arial", size=11)

    for line in md_text.splitlines():

        pdf.multi_cell(0, 5, line)

    pdf_bytes = BytesIO()

    pdf.output(pdf_bytes)

    pdf_bytes.seek(0)

    return pdf_bytes.getvalue()

```



### 5.3 Deep Summary Prompt (2000‚Äì3000 words)



```python

def build_deep_summary_prompt(doc_text: str, lang: str) -> str:

    language_instruction = (

        "Write the entire output in English."

        if lang == "en"

        else "Ë´ã‰ΩøÁî®ÁπÅÈ´î‰∏≠ÊñáÊí∞ÂØ´Êï¥‰ªΩËº∏Âá∫„ÄÇ"

    )



    base = f"""

You are an elite strategic auditor, legal/financial analyst, and knowledge architect.

{language_instruction}



You are given the full content of a document. Your tasks:



1. Produce a **deep, structured Markdown report** summarizing and analyzing the document.

2. Length: **2,000‚Äì3,000 words** (do not be shorter than ~2,000 words unless absolutely necessary).

3. Use the following structure (Markdown headings):



# Executive Overview

- Concise summary of purpose, audience, and domain.



## Key Themes & Objectives

- Bullet list of central goals, decisions, or arguments.



## Detailed Section-by-Section Analysis

- For each major section, provide:

  - What it says

  - Why it matters

  - Risks or opportunities



## Critical Risks, Gaps, and Red Flags

- Legal, compliance, financial, strategic, and operational risks.

- Ambiguities or missing information that might be important.



## Actionable Recommendations

- Concrete steps, prioritized.

- Who should care and why.



## Stakeholder-Specific Views

- How different stakeholders (e.g., executives, auditors, regulators, clients) should interpret the document.



## Glossary of Key Terms (if applicable)

- Short, precise definitions.



Constraints:

- Use Markdown headings (#, ##, ###) and bullet lists.

- Be analytical, not just descriptive.

- When in doubt, **over-explain** key concepts rather than under-explaining.



Now, here is the document content (possibly truncated for length):



[DOCUMENT START]

{doc_text[:100000]}

[DOCUMENT END]

"""

    return base.strip()

```



### 5.4 New Tab: File Transform & Deep Summary



```python

def tab_file_transform_deep_summary():

    st.markdown(f"## {t('tab_file_transform')}")

    st.markdown(

        '<div class="glass-panel">',

        unsafe_allow_html=True,

    )



    uploaded = st.file_uploader(

        t("upload_label"),

        type=["pdf", "docx", "txt"],

        key="file_transform_uploader",

    )



    output_format = st.radio(

        t("output_format"),

        [t("format_markdown"), t("format_pdf")],

        horizontal=True,

        key="output_format_choice",

    )



    if uploaded is not None:

        if st.button(t("run_summary"), type="primary"):

            with st.spinner("Extracting text and generating deep summary‚Ä¶"):

                raw_text = extract_text(uploaded)

                if not raw_text.strip():

                    st.error("No readable text extracted from the file.")

                    return



                provider, model_id, max_tokens, temperature, system_prompt = model_controls()

                lang = st.session_state.get("ui_lang", "en")



                prompt = build_deep_summary_prompt(raw_text, lang)

                output = call_llm(

                    provider=provider,

                    model=model_id,

                    system_prompt=system_prompt,

                    user_messages=[{"role": "user", "content": prompt}],

                    max_tokens=max_tokens,

                    temperature=temperature,

                )

                if not output:

                    return



                # Save to session for RAG-style chat

                st.session_state["latest_file_text"] = raw_text

                st.session_state["latest_file_summary_md"] = output

                st.session_state["latest_file_name"] = uploaded.name



                # Show markdown summary

                st.markdown("### üìÑ Deep Summary (Markdown)")

                st.markdown(output)



                # Download options

                if output_format == t("format_markdown"):

                    st.download_button(

                        "Download Markdown",

                        data=output.encode("utf-8"),

                        file_name=f"{uploaded.name}.summary.md",

                        mime="text/markdown",

                    )

                else:

                    pdf_bytes = markdown_to_pdf_bytes(output)

                    st.download_button(

                        "Download PDF",

                        data=pdf_bytes,

                        file_name=f"{uploaded.name}.summary.pdf",

                        mime="application/pdf",

                    )



    # Chat with file

    if "latest_file_text" in st.session_state:

        st.markdown("---")

        st.markdown(f"### üí¨ {t('chat_with_file')} ‚Äî {st.session_state.get('latest_file_name', '')}")

        user_q = st.text_area(t("user_prompt"), key="file_chat_prompt")

        if st.button("Ask the file"):

            provider, model_id, max_tokens, temperature, system_prompt = model_controls()

            full_context = f"""

You are given a source document and a long-form summary of that document.



[ORIGINAL DOCUMENT]

{st.session_state['latest_file_text'][:60000]}



[SUMMARY]

{st.session_state['latest_file_summary_md'][:40000]}



Answer the user's question strictly grounded in these sources.

If the answer is not supported by the information above, say so clearly.

"""

            question = user_q.strip()

            if not question:

                st.warning("Please enter a question.")

                return



            with st.spinner("Thinking with the document‚Ä¶"):

                answer = call_llm(

                    provider=provider,

                    model=model_id,

                    system_prompt=system_prompt,

                    user_messages=[

                        {"role": "user", "content": full_context},

                        {"role": "user", "content": question},

                    ],

                    max_tokens=max_tokens,

                    temperature=temperature,

                )

            st.markdown("#### Answer")

            st.markdown(answer or "_No answer produced._")



    st.markdown("</div>", unsafe_allow_html=True)

```



---



## 6. Other Tabs (Kept Features, Simplified)



Below are minimal but functional skeletons for the 4 original modules, using the same `call_llm` and controls. You can extend prompts as deeply as you like.



### 6.1 File Intelligence (Single File)



```python

def tab_file_intelligence():

    st.markdown(f"## {t('tab_file_intel')}")

    st.markdown('<div class="glass-panel">', unsafe_allow_html=True)

    up = st.file_uploader(

        t("upload_label"),

        type=["pdf", "docx", "txt", "md"],

        key="file_intel_uploader",

    )

    if up is not None and st.button("Analyze File"):

        with st.spinner("Analyzing file‚Ä¶"):

            text = extract_text(up)

            provider, model_id, max_tokens, temperature, system_prompt = model_controls()

            lang = st.session_state.get("ui_lang", "en")



            language_instruction = (

                "Write the output in English."

                if lang == "en"

                else "Ë´ã‰ΩøÁî®ÁπÅÈ´î‰∏≠ÊñáÊí∞ÂØ´Ëº∏Âá∫„ÄÇ"

            )

            prompt = f"""

You are an elite strategic auditor and document analyst.

{language_instruction}



Analyze the following document and produce a structured Markdown report.



[DOCUMENT START]

{text[:100000]}

[DOCUMENT END]

"""

            result = call_llm(

                provider=provider,

                model=model_id,

                system_prompt=system_prompt,

                user_messages=[{"role": "user", "content": prompt}],

                max_tokens=max_tokens,

                temperature=temperature,

            )

            st.markdown("### Analysis")

            st.markdown(result or "_No output._")

    st.markdown("</div>", unsafe_allow_html=True)

```



### 6.2 Multi-File Synthesis



```python

def tab_multi_file_synthesis():

    st.markdown(f"## {t('tab_multi_file')}")

    st.markdown('<div class="glass-panel">', unsafe_allow_html=True)

    files = st.file_uploader(

        "Upload multiple files (PDF/DOCX/TXT/MD)",

        type=["pdf", "docx", "txt", "md"],

        accept_multiple_files=True,

        key="multi_files",

    )

    if files and st.button("Combine & Analyze"):

        with st.spinner("Combining and analyzing files‚Ä¶"):

            assembled = []

            for f in files:

                content = extract_text(f)

                assembled.append(f"--- START FILE: {f.name} ---\n{content}\n--- END FILE: {f.name} ---\n")

            combined = "\n".join(assembled)[:150000]



            provider, model_id, max_tokens, temperature, system_prompt = model_controls()

            lang = st.session_state.get("ui_lang", "en")

            language_instruction = (

                "Write the output in English."

                if lang == "en"

                else "Ë´ã‰ΩøÁî®ÁπÅÈ´î‰∏≠ÊñáÊí∞ÂØ´Ëº∏Âá∫„ÄÇ"

            )



            prompt = f"""

You are an advanced synthesis engine (strategic auditor / consultant).

{language_instruction}



You are given multiple documents enclosed in START/END FILE markers.

Treat them as a unified knowledge base.



Tasks:

- Compare and contrast documents.

- Identify trends, inconsistencies, and deltas across files.

- Provide a structured Markdown report with:

  - Executive Summary

  - Cross-Document Comparisons

  - Key Risks / Gaps

  - Recommended Next Actions.



[DOCUMENTS]

{combined}

"""

            result = call_llm(

                provider=provider,

                model=model_id,

                system_prompt=system_prompt,

                user_messages=[{"role": "user", "content": prompt}],

                max_tokens=max_tokens,

                temperature=temperature,

            )

            st.markdown("### Synthesis Report")

            st.markdown(result or "_No output._")

    st.markdown("</div>", unsafe_allow_html=True)

```



### 6.3 Smart Replace (Template Filling)



```python

def tab_smart_replace():

    st.markdown(f"## {t('tab_smart_replace')}")

    st.markdown('<div class="glass-panel">', unsafe_allow_html=True)

    col1, col2 = st.columns(2)

    with col1:

        template_text = st.text_area("Template (with placeholders like [Client Name])", height=260)

    with col2:

        context_text = st.text_area("Context / Raw Data Source", height=260)



    instructions = st.text_area(

        "Natural language instructions (tone, style, constraints)",

        value="Fill in all placeholders using the context. Keep it legally precise and professional.",

    )



    if st.button("Run Smart Replace"):

        provider, model_id, max_tokens, temperature, system_prompt = model_controls()

        lang = st.session_state.get("ui_lang", "en")

        language_instruction = (

            "Write the output in English."

            if lang == "en"

            else "Ë´ã‰ΩøÁî®ÁπÅÈ´î‰∏≠ÊñáÊí∞ÂØ´Ëº∏Âá∫„ÄÇ"

        )



        prompt = f"""

You are a legal and business drafting expert.



{language_instruction}



Here is a template document with placeholders like [Client Name], [Effective Date], etc.



[TEMPLATE]

{template_text}



Here is unstructured context that contains the values you should infer:

[CONTEXT]

{context_text}



Instructions from the user:

{instructions}



Tasks:

- Infer values for all placeholders from CONTEXT.

- Replace placeholders carefully and adjust surrounding text so it reads naturally.

- Do not invent facts not reasonably supported by the context.

- Output only the **completed template** in Markdown.

"""

        with st.spinner("Generating filled template‚Ä¶"):

            result = call_llm(

                provider=provider,

                model=model_id,

                system_prompt=system_prompt,

                user_messages=[{"role": "user", "content": prompt}],

                max_tokens=max_tokens,

                temperature=temperature,

            )

        st.markdown("### Completed Template")

        st.markdown(result or "_No output._")

    st.markdown("</div>", unsafe_allow_html=True)

```



### 6.4 AI Note Keeper



```python

def tab_ai_note_keeper():

    st.markdown(f"## {t('tab_note_keeper')}")

    st.markdown('<div class="glass-panel">', unsafe_allow_html=True)



    raw_note = st.text_area("Your raw notes / brain dump", height=240, key="note_raw")

    col1, col2, col3, col4, col5 = st.columns(5)

    action = None

    if col1.button("Format"):

        action = "format"

    if col2.button("Tasks"):

        action = "tasks"

    if col3.button("Fix"):

        action = "fix"

    if col4.button("Summary"):

        action = "summary"

    if col5.button("Expand"):

        action = "expand"



    if action and raw_note.strip():

        provider, model_id, max_tokens, temperature, system_prompt = model_controls()

        lang = st.session_state.get("ui_lang", "en")

        language_instruction = (

            "Write the output in English."

            if lang == "en"

            else "Ë´ã‰ΩøÁî®ÁπÅÈ´î‰∏≠ÊñáÊí∞ÂØ´Ëº∏Âá∫„ÄÇ"

        )



        prompt_map = {

            "format": "Reformat these notes into clean Markdown with clear headings and bullet points.",

            "tasks": "Extract all actionable tasks and output them as a checklist (- [ ] ...).",

            "fix": "Fix grammar, spelling, and clarity, keeping a business-professional tone.",

            "summary": "Write a concise TL;DR summary followed by a structured outline.",

            "expand": "Expand terse bullets into fleshed-out paragraphs with examples.",

        }

        prompt = f"""

You are an expert personal knowledge manager.



{language_instruction}



User's raw notes:

{raw_note}



Task: {prompt_map[action]}



Output only the transformed notes in Markdown.

"""

        with st.spinner("Transforming notes‚Ä¶"):

            result = call_llm(

                provider=provider,

                model=model_id,

                system_prompt=system_prompt,

                user_messages=[{"role": "user", "content": prompt}],

                max_tokens=max_tokens,

                temperature=temperature,

            )

        st.markdown("### Transformed Notes")

        st.markdown(result or "_No output._")

    st.markdown("</div>", unsafe_allow_html=True)

```



---



## 7. Putting It All Together: `app.py` Skeleton



```python

import streamlit as st



# (import everything above‚Ä¶)



def main():

    # Initialize session defaults

    st.set_page_config(

        page_title="AuditFlow AI ¬∑ Masterpiece Edition",

        layout="wide",

    )

    if "ui_lang" not in st.session_state:

        st.session_state.ui_lang = "en"

    if "dark_mode" not in st.session_state:

        st.session_state.dark_mode = True

    if "artist_style_key" not in st.session_state:

        st.session_state.artist_style_key = "van_gogh"



    # Sidebar: global controls

    with st.sidebar:

        st.markdown("## üé® Masterpiece Controls")

        lang = st.radio("Language / Ë™ûË®Ä", ["English", "ÁπÅÈ´î‰∏≠Êñá"], key="lang_radio")

        st.session_state.ui_lang = "en" if lang == "English" else "zh"



        dark_mode = st.toggle("Dark mode", value=True, key="dark_mode_toggle")

        st.session_state.dark_mode = dark_mode



        active_style = style_selector_ui()

        provider = api_keys_section()

        provider, model_id, max_tokens, temperature, system_prompt = model_controls()



    # Apply theme

    apply_theme(active_style, st.session_state.dark_mode)



    # Main header

    st.markdown(f"<h1 class='accent-title'>{t('app_title')}</h1>", unsafe_allow_html=True)

    st.markdown(t("subtitle"))



    # Tabs

    tab1, tab2, tab3, tab4, tab5 = st.tabs([

        t("tab_file_transform"),

        t("tab_file_intel"),

        t("tab_multi_file"),

        t("tab_smart_replace"),

        t("tab_note_keeper"),

    ])



    with tab1:

        tab_file_transform_deep_summary()

    with tab2:

        tab_file_intelligence()

    with tab3:

        tab_multi_file_synthesis()

    with tab4:

        tab_smart_replace()

    with tab5:

        tab_ai_note_keeper()



if __name__ == "__main__":

    main()

```



This gives you:



- Agentic flows (file ingest ‚Üí analysis ‚Üí RAG Q&A).

- Wow-style painter UI with light/dark, EN/ÁπÅ‰∏≠, and jackpot styling.

- User control over model, prompt, tokens before running agents.

- BYOK model with safe env-key usage (no key display if from env).

- Sample Grok (XAI) integration built exactly on your sample API snippet concept.



---



## 8. 20 Follow-Up Questions



1. For the **2000‚Äì3000 word summary**, do you want a **hard length constraint** (model forced to reach that size) or a more flexible ‚Äúaim for that range‚Äù as it is now?

2. Should the **output language** always match the **UI language**, or do you want a **separate toggle** for ‚ÄúUI language‚Äù vs ‚Äúanalysis output language‚Äù?

3. In the new **File Transform & Deep Summary** tab, do you also want the option to **download the original extracted plain text** alongside the markdown/PDF?

4. For **RAG-style chat**, do you prefer a **simple full-context injection** (current approach) or should we integrate an actual **chunking + similarity search** pipeline (e.g., local embeddings) for larger documents?

5. Do you want the **chat history with each file** to be persisted per session (multi-turn context) or should each question be treated as **stateless**?

6. For **Multi-File Synthesis**, should users be able to **tag files by category** (e.g., ‚ÄúQ1 report‚Äù, ‚ÄúContract‚Äù, ‚ÄúPolicy‚Äù) and have the model reason using those tags explicitly?

7. Would you like an explicit **"Risk Heatmap" section** (e.g., Low/Medium/High with colored chips) in the summary outputs, which we can enforce via prompt instructions?

8. For the **Smart Replace** module, do you need **export to DOCX/PDF** in addition to markdown, or is markdown sufficient for now?

9. Should **AI Note Keeper** support **multiple saved notes** with titles (mini knowledge base) or is a single working area per session sufficient?

10. Do you want a specific **default model mapping per tab** (e.g., Gemini 3 Flash for synthesis, Grok for creative reasoning, etc.) or keep a single global model selection?

11. For **API usage safety**, would you like a simple **token/usage estimator** shown before each run to help users understand potential cost?

12. Are there any **compliance or PII constraints** (e.g., never send certain fields to the LLM) that require **in-browser redaction** before the text goes to the API?

13. Should the **‚ÄúJackpot‚Äù style** also randomize between **light/dark** or only shuffle painter styles while preserving the current theme?

14. Do you want **per-style overrides** of font sizes and spacing (e.g., more compact for Banksy, more airy for Monet) or keep layout consistent for readability?

15. For **Traditional Chinese**, do you prefer a **Taiwan-style vocabulary and punctuation** or a more generic Traditional Chinese style?

16. Would you like an optional **‚Äúauditor persona selection‚Äù** (legal / financial / strategic / technical) that changes the system prompt templates and the type of analysis?

17. Is support for **image-containing PDFs** (OCR) important for your first version, or can we assume text-based PDFs only for now?

18. Should we add a **‚Äúraw LLM playground‚Äù tab** where users can freely edit system prompt, messages, model, and tokens without any document context?

19. Do you want a **simple telemetry panel** (client-side only) that counts how many calls were made per provider/model in the current session?

20. Are there any **specific famous painters or visual brands** you want added or replaced in the current list of 20 styles to better match your product identity?

„Éï„Ç°„Ç§„É´„ÅåÈÅ∏Êäû„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì„Éï„Ç°„Ç§„É´„ÅåÈÅ∏Êäû„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì
Super, please create app.py, advanced agents.yaml with 31 agents that is handy for FDA in traditional chinese. Ending with 20 comprehensive follow up questions.

```python

# app.py

# AuditFlow AI ¬∑ Masterpiece Edition (FDA-focused, Agentic, Painter-Themed)

# Designed for Streamlit on Hugging Face Spaces

#

# Features:

# - Wow UI with 20 painter styles + Jackpot selector

# - Light/Dark theme

# - English / ÁπÅÈ´î‰∏≠Êñá UI

# - Agent selection from advanced_agents.yaml (31 FDA-oriented agents, in Traditional Chinese)

# - BYOK for OpenAI / Gemini / Anthropic / XAI (Grok), with env fallback (hidden)

# - File Transform & Deep Summary tab (PDF/DOCX/TXT ‚Üí Markdown/PDF + 2,000‚Äì3,000 word summary)

# - File Intelligence, Multi-File Synthesis, Smart Replace, AI Note Keeper

# - Model selection, max tokens, temperature, customizable system prompt before executing agents



import os

import time

import random

from dataclasses import dataclass

from io import BytesIO

from typing import List, Dict, Any



import streamlit as st

import yaml

from openai import OpenAI

import google.generativeai as genai

import anthropic

from xai_sdk import Client as XAIClient

from xai_sdk.chat import user as xai_user, system as xai_system



import docx2txt

from PyPDF2 import PdfReader

from fpdf import FPDF





# =========================

#  Localization

# =========================



UI_TEXT = {

    "en": {

        "app_title": "AuditFlow AI ¬∑ Masterpiece Edition (FDA)",

        "subtitle": "FDA-oriented agentic document intelligence with painterly themes.",

        "tab_file_transform": "File Transform & Deep Summary",

        "tab_file_intel": "File Intelligence",

        "tab_multi_file": "Multi-File Synthesis",

        "tab_smart_replace": "Smart Replace",

        "tab_note_keeper": "AI Note Keeper",

        "upload_label": "Upload a document (PDF, DOCX, TXT):",

        "output_format": "Transform file into:",

        "format_markdown": "Markdown (.md)",

        "format_pdf": "PDF (.pdf)",

        "run_summary": "Generate 2,000‚Äì3,000 word Masterpiece summary",

        "chat_with_file": "Chat with this file",

        "api_key_section": "API Keys (browser-only, never sent to any server except LLM provider)",

        "provider": "Provider",

        "model": "Model",

        "custom_prompt": "Custom system prompt",

        "max_tokens": "Max tokens",

        "temperature": "Temperature",

        "user_prompt": "Your question / instruction",

        "agent_select": "FDA Agent (from advanced_agents.yaml)",

    },

    "zh": {

        "app_title": "AuditFlow AI ¬∑ Â§ßÂ∏´ÂÇë‰ΩúÁâàÔºàFDA Â∞àÁî®Ôºâ",

        "subtitle": "Èù¢Âêë FDA Â†±Ë¶èËàáÂêàË¶èÈúÄÊ±ÇÁöÑ‰ª£ÁêÜÂºèÊñá‰ª∂Êô∫ÊÖßÁ≥ªÁµ±ÔºåÁµêÂêàËóùË°ìÈ¢®Ê†ºÈ´îÈ©ó„ÄÇ",

        "tab_file_transform": "Ê™îÊ°àËΩâÊèõËàáÊ∑±Â∫¶ÊëòË¶Å",

        "tab_file_intel": "ÂñÆ‰∏ÄÊñá‰ª∂ÂàÜÊûê",

        "tab_multi_file": "Â§öÊñá‰ª∂Á∂úÂêàÂàÜÊûê",

        "tab_smart_replace": "Êô∫ÊÖßÁØÑÊú¨Â°´ÂØ´",

        "tab_note_keeper": "AI Á≠ÜË®òÁÆ°ÁêÜÂì°",

        "upload_label": "‰∏äÂÇ≥Êñá‰ª∂ÔºàPDF„ÄÅDOCX„ÄÅTXTÔºâÔºö",

        "output_format": "Â∞áÊ™îÊ°àËΩâÊèõÁÇ∫Ôºö",

        "format_markdown": "Markdown (.md)",

        "format_pdf": "PDF (.pdf)",

        "run_summary": "Áî¢Áîü 2,000‚Äì3,000 Â≠óÊ∑±Â∫¶ÊëòË¶ÅÔºàMarkdownÔºâ",

        "chat_with_file": "ÈáùÂ∞çÊ≠§Êñá‰ª∂ÁôºÂïè",

        "api_key_section": "API ÈáëÈë∞ÔºàÂÉÖÂú®Êú¨Ê©üÁÄèË¶ΩÂô®‰∏≠‰ΩøÁî®ÔºåÂÉÖÈÄÅÂæÄ LLM ‰æõÊáâÂïÜÔºâ",

        "provider": "ÊúçÂãôÊèê‰æõËÄÖ",

        "model": "Ê®°Âûã",

        "custom_prompt": "Ëá™Ë®ÇÁ≥ªÁµ±ÊèêÁ§∫ÔºàSystem PromptÔºâ",

        "max_tokens": "ÊúÄÂ§ß Token Êï∏",

        "temperature": "Ê∫´Â∫¶",

        "user_prompt": "‰Ω†ÁöÑÂïèÈ°å / Êåá‰ª§",

        "agent_select": "FDA ‰ª£ÁêÜ‰∫∫Ôºà‰æÜËá™ advanced_agents.yamlÔºâ",

    },

}





def t(key: str) -> str:

    lang = st.session_state.get("ui_lang", "en")

    return UI_TEXT.get(lang, UI_TEXT["en"]).get(key, key)





# =========================

#  Painter Styles

# =========================



@dataclass

class ArtistStyle:

    key: str

    display_name: str

    painter: str

    bg_gradient_light: str

    bg_gradient_dark: str

    panel_bg_rgba: str

    accent_color: str

    accent_soft: str

    font_family: str





ARTIST_STYLES: List[ArtistStyle] = [

    ArtistStyle(

        key="van_gogh",

        display_name="Starry Night",

        painter="Vincent van Gogh",

        bg_gradient_light="linear-gradient(135deg,#fdfbfb 0%,#ebedee 100%)",

        bg_gradient_dark="linear-gradient(135deg,#0f172a 0%,#1e293b 100%)",

        panel_bg_rgba="rgba(15, 23, 42, 0.75)",

        accent_color="#facc15",

        accent_soft="#fef9c3",

        font_family="'DM Sans', system-ui, -apple-system, BlinkMacSystemFont, sans-serif",

    ),

    ArtistStyle(

        key="monet",

        display_name="Water Lilies",

        painter="Claude Monet",

        bg_gradient_light="linear-gradient(135deg,#e0f4ff 0%,#f9f7ff 100%)",

        bg_gradient_dark="linear-gradient(135deg,#0b1120 0%,#1d2233 100%)",

        panel_bg_rgba="rgba(15, 23, 42, 0.70)",

        accent_color="#22c55e",

        accent_soft="#dcfce7",

        font_family="'Playfair Display', Georgia, 'Times New Roman', serif",

    ),

    ArtistStyle(

        key="picasso",

        display_name="Cubist Geometry",

        painter="Pablo Picasso",

        bg_gradient_light="linear-gradient(135deg,#fdfbfb 0%,#ebedee 40%,#fee2e2 100%)",

        bg_gradient_dark="linear-gradient(135deg,#020617 0%,#111827 50%,#1f2933 100%)",

        panel_bg_rgba="rgba(15, 23, 42, 0.80)",

        accent_color="#f97316",

        accent_soft="#ffedd5",

        font_family="'Space Grotesk', system-ui, sans-serif",

    ),

    ArtistStyle(

        key="da_vinci",

        display_name="Renaissance Studio",

        painter="Leonardo da Vinci",

        bg_gradient_light="linear-gradient(135deg,#faf5e4 0%,#fef9c3 100%)",

        bg_gradient_dark="linear-gradient(135deg,#1c1917 0%,#292524 100%)",

        panel_bg_rgba="rgba(24, 24, 27, 0.85)",

        accent_color="#fbbf24",

        accent_soft="#fef3c7",

        font_family="'Crimson Text', Georgia, 'Times New Roman', serif",

    ),

    ArtistStyle(

        key="michelangelo",

        display_name="Sistine Ceiling",

        painter="Michelangelo",

        bg_gradient_light="linear-gradient(135deg,#e5e7eb 0%,#f9fafb 100%)",

        bg_gradient_dark="linear-gradient(135deg,#111827 0%,#020617 100%)",

        panel_bg_rgba="rgba(15, 23, 42, 0.78)",

        accent_color="#60a5fa",

        accent_soft="#dbeafe",

        font_family="'Playfair Display', Georgia, serif",

    ),

    ArtistStyle(

        key="rembrandt",

        display_name="Chiaroscuro",

        painter="Rembrandt",

        bg_gradient_light="linear-gradient(135deg,#fef3c7 0%,#fed7aa 100%)",

        bg_gradient_dark="linear-gradient(135deg,#0b1120 0%,#1f2937 100%)",

        panel_bg_rgba="rgba(15, 23, 42, 0.9)",

        accent_color="#f97316",

        accent_soft="#ffedd5",

        font_family="'Merriweather', Georgia, serif",

    ),

    ArtistStyle(

        key="klimt",

        display_name="Golden Mosaic",

        painter="Gustav Klimt",

        bg_gradient_light="linear-gradient(135deg,#fef3c7 0%,#facc15 50%,#fee2e2 100%)",

        bg_gradient_dark="linear-gradient(135deg,#1c1917 0%,#3f3f46 100%)",

        panel_bg_rgba="rgba(24, 24, 27, 0.85)",

        accent_color="#eab308",

        accent_soft="#fef9c3",

        font_family="'Cormorant Garamond', Georgia, serif",

    ),

    ArtistStyle(

        key="matisse",

        display_name="Cut-Outs",

        painter="Henri Matisse",

        bg_gradient_light="linear-gradient(135deg,#fee2e2 0%,#f9a8d4 50%,#bfdbfe 100%)",

        bg_gradient_dark="linear-gradient(135deg,#020617 0%,#0f172a 100%)",

        panel_bg_rgba="rgba(15, 23, 42, 0.9)",

        accent_color="#ec4899",

        accent_soft="#fce7f3",

        font_family="'Fredoka', system-ui, sans-serif",

    ),

    ArtistStyle(

        key="dali",

        display_name="Surreal Desert",

        painter="Salvador Dal√≠",

        bg_gradient_light="linear-gradient(135deg,#fef3c7 0%,#fde68a 40%,#bfdbfe 100%)",

        bg_gradient_dark="linear-gradient(135deg,#111827 0%,#1f2937 100%)",

        panel_bg_rgba="rgba(17, 24, 39, 0.85)",

        accent_color="#f97316",

        accent_soft="#ffedd5",

        font_family="'IBM Plex Sans', system-ui, sans-serif",

    ),

    ArtistStyle(

        key="warhol",

        display_name="Pop Factory",

        painter="Andy Warhol",

        bg_gradient_light="linear-gradient(135deg,#f9a8d4 0%,#f97316 40%,#22c55e 70%,#38bdf8 100%)",

        bg_gradient_dark="linear-gradient(135deg,#020617 0%,#111827 100%)",

        panel_bg_rgba="rgba(15, 23, 42, 0.9)",

        accent_color="#ec4899",

        accent_soft="#fee2e2",

        font_family="'Poppins', system-ui, sans-serif",

    ),

    ArtistStyle(

        key="hokusai",

        display_name="Great Wave",

        painter="Hokusai",

        bg_gradient_light="linear-gradient(135deg,#e0f2fe 0%,#bfdbfe 40%,#f1f5f9 100%)",

        bg_gradient_dark="linear-gradient(135deg,#020617 0%,#0f172a 100%)",

        panel_bg_rgba="rgba(15, 23, 42, 0.9)",

        accent_color="#0ea5e9",

        accent_soft="#dbeafe",

        font_family="'Noto Sans TC', system-ui, sans-serif",

    ),

    ArtistStyle(

        key="frida",

        display_name="Vivid Blossoms",

        painter="Frida Kahlo",

        bg_gradient_light="linear-gradient(135deg,#fee2e2 0%,#fecaca 40%,#bbf7d0 100%)",

        bg_gradient_dark="linear-gradient(135deg,#0f172a 0%,#1e293b 100%)",

        panel_bg_rgba="rgba(15, 23, 42, 0.85)",

        accent_color="#f97316",

        accent_soft="#ffedd5",

        font_family="'Josefin Sans', system-ui, sans-serif",

    ),

    ArtistStyle(

        key="banksy",

        display_name="Street Brutalism",

        painter="Banksy",

        bg_gradient_light="linear-gradient(135deg,#e5e7eb 0%,#9ca3af 40%,#111827 100%)",

        bg_gradient_dark="linear-gradient(135deg,#020617 0%,#111827 100%)",

        panel_bg_rgba="rgba(17, 24, 39, 0.95)",

        accent_color="#f97316",

        accent_soft="#fee2e2",

        font_family="'Roboto Mono', ui-monospace, SFMono-Regular, Menlo, Monaco, monospace",

    ),

    ArtistStyle(

        key="rothko",

        display_name="Color Fields",

        painter="Mark Rothko",

        bg_gradient_light="linear-gradient(135deg,#fecaca 0%,#fed7aa 40%,#fef9c3 100%)",

        bg_gradient_dark="linear-gradient(135deg,#111827 0%,#1f2937 100%)",

        panel_bg_rgba="rgba(17, 24, 39, 0.9)",

        accent_color="#fb7185",

        accent_soft="#fee2e2",

        font_family="'Work Sans', system-ui, sans-serif",

    ),

    ArtistStyle(

        key="chagall",

        display_name="Dreamscapes",

        painter="Marc Chagall",

        bg_gradient_light="linear-gradient(135deg,#e0e7ff 0%,#f5d0fe 40%,#cffafe 100%)",

        bg_gradient_dark="linear-gradient(135deg,#020617 0%,#0f172a 100%)",

        panel_bg_rgba="rgba(15, 23, 42, 0.9)",

        accent_color="#a855f7",

        accent_soft="#ede9fe",

        font_family="'Quicksand', system-ui, sans-serif",

    ),

    ArtistStyle(

        key="basquiat",

        display_name="Neo-Expressionism",

        painter="Jean-Michel Basquiat",

        bg_gradient_light="linear-gradient(135deg,#fee2e2 0%,#f97316 30%,#22c55e 60%,#38bdf8 100%)",

        bg_gradient_dark="linear-gradient(135deg,#020617 0%,#0f172a 100%)",

        panel_bg_rgba="rgba(15, 23, 42, 0.95)",

        accent_color="#facc15",

        accent_soft="#fef3c7",

        font_family="'Inter', system-ui, sans-serif",

    ),

    ArtistStyle(

        key="turner",

        display_name="Storm Light",

        painter="J. M. W. Turner",

        bg_gradient_light="linear-gradient(135deg,#fef3c7 0%,#fde68a 50%,#bfdbfe 100%)",

        bg_gradient_dark="linear-gradient(135deg,#020617 0%,#111827 100%)",

        panel_bg_rgba="rgba(15, 23, 42, 0.85)",

        accent_color="#f59e0b",

        accent_soft="#fef3c7",

        font_family="'DM Serif Display', Georgia, serif",

    ),

    ArtistStyle(

        key="vermeer",

        display_name="Soft Interior",

        painter="Johannes Vermeer",

        bg_gradient_light="linear-gradient(135deg,#e5e7eb 0%,#e0f2fe 50%,#fef9c3 100%)",

        bg_gradient_dark="linear-gradient(135deg,#020617 0%,#0f172a 100%)",

        panel_bg_rgba="rgba(15, 23, 42, 0.85)",

        accent_color="#38bdf8",

        accent_soft="#dbeafe",

        font_family="'Lora', Georgia, serif",

    ),

    ArtistStyle(

        key="cezanne",

        display_name="Mountain Geometry",

        painter="Paul C√©zanne",

        bg_gradient_light="linear-gradient(135deg,#e0f2fe 0%,#bbf7d0 50%,#fee2e2 100%)",

        bg_gradient_dark="linear-gradient(135deg,#020617 0%,#0f172a 100%)",

        panel_bg_rgba="rgba(15, 23, 42, 0.9)",

        accent_color="#22c55e",

        accent_soft="#dcfce7",

        font_family="'Source Sans 3', system-ui, sans-serif",

    ),

    ArtistStyle(

        key="pollock",

        display_name="Action Painting",

        painter="Jackson Pollock",

        bg_gradient_light="linear-gradient(135deg,#f1f5f9 0%,#e5e7eb 30%,#fecaca 60%,#bef264 100%)",

        bg_gradient_dark="linear-gradient(135deg,#020617 0%,#111827 100%)",

        panel_bg_rgba="rgba(15, 23, 42, 0.95)",

        accent_color="#f97316",

        accent_soft="#fed7aa",

        font_family="'Manrope', system-ui, sans-serif",

    ),

]





def apply_theme(style: ArtistStyle, dark_mode: bool):

    bg = style.bg_gradient_dark if dark_mode else style.bg_gradient_light

    panel = style.panel_bg_rgba

    text_color = "#e5e7eb" if dark_mode else "#020617"



    css = f"""

    <style>

    html, body, [data-testid="stAppViewContainer"] {{

        background: {bg} !important;

        background-attachment: fixed;

        font-family: {style.font_family};

        color: {text_color};

    }}

    .glass-panel {{

        background: {panel};

        backdrop-filter: blur(18px);

        -webkit-backdrop-filter: blur(18px);

        border-radius: 20px;

        border: 1px solid rgba(255,255,255,0.18);

        padding: 1.25rem 1.5rem;

        margin-bottom: 1.5rem;

    }}

    .accent-title {{

        color: {style.accent_color};

    }}

    .accent-chip {{

        background: {style.accent_soft};

        color: #111827;

        border-radius: 9999px;

        padding: 0.15rem 0.7rem;

        font-size: 0.75rem;

        font-weight: 500;

        display: inline-flex;

        align-items: center;

        gap: 0.25rem;

    }}

    textarea, .stTextInput > div > div > input {{

        background: rgba(15,23,42,0.75) !important;

        color: #e5e7eb !important;

    }}

    </style>

    """

    st.markdown(css, unsafe_allow_html=True)





def style_selector_ui() -> ArtistStyle:

    st.markdown("### üé® Masterpiece Style Jackpot")

    style_keys = [s.key for s in ARTIST_STYLES]

    current_style_key = st.session_state.get("artist_style_key", "van_gogh")



    col1, col2 = st.columns([3, 1])

    with col1:

        selected_key = st.selectbox(

            "Style",

            options=style_keys,

            index=style_keys.index(current_style_key) if current_style_key in style_keys else 0,

            format_func=lambda k: next(s.display_name for s in ARTIST_STYLES if s.key == k),

            key="artist_style_dropdown",

        )

    with col2:

        if st.button("Inspire Me (Jackpot)"):

            placeholder = st.empty()

            for _ in range(15):

                rand_key = random.choice(style_keys)

                st.session_state.artist_style_key = rand_key

                placeholder.write(

                    f"üé∞ üé® {next(s.display_name for s in ARTIST_STYLES if s.key == rand_key)}"

                )

                time.sleep(0.06)

            placeholder.empty()



    st.session_state.artist_style_key = st.session_state.get("artist_style_key", selected_key)

    active_style = next(s for s in ARTIST_STYLES if s.key == st.session_state.artist_style_key)

    return active_style





# =========================

#  Agents (from YAML)

# =========================



def load_agents(path: str = "advanced_agents.yaml") -> List[Dict[str, Any]]:

    try:

        with open(path, "r", encoding="utf-8") as f:

            data = yaml.safe_load(f)

        return data.get("agents", [])

    except Exception as e:

        st.sidebar.error(f"Failed to load agents YAML: {e}")

        return []





def agent_selector_ui(agents: List[Dict[str, Any]]) -> Dict[str, Any]:

    if not agents:

        st.sidebar.warning("No agents loaded from advanced_agents.yaml.")

        return {}



    st.sidebar.markdown(f"### ü§ñ {t('agent_select')}")

    ids = [a["id"] for a in agents]



    def label_func(agent_id: str) -> str:

        a = next(ag for ag in agents if ag["id"] == agent_id)

        return a.get("display_name_zh", agent_id)



    default_idx = 0

    if "selected_agent_id" in st.session_state:

        try:

            default_idx = ids.index(st.session_state["selected_agent_id"])

        except ValueError:

            default_idx = 0



    selected_id = st.sidebar.selectbox(

        "Agent",

        options=ids,

        index=default_idx,

        format_func=label_func,

        key="agent_selectbox",

    )

    selected_agent = next(a for a in agents if a["id"] == selected_id)



    # When agent changes, overwrite model & prompt defaults

    if st.session_state.get("selected_agent_id") != selected_id:

        st.session_state["selected_agent_id"] = selected_id

        # Defaults from agent config

        st.session_state["llm_provider"] = selected_agent.get("default_provider", "Gemini")

        st.session_state["llm_model_id"] = selected_agent.get("default_model", "gemini-3-flash")

        st.session_state["llm_max_tokens"] = selected_agent.get("default_max_tokens", 4096)

        st.session_state["llm_temperature"] = selected_agent.get("default_temperature", 0.3)

        st.session_state["llm_system_prompt"] = selected_agent.get(

            "system_prompt_zh",

            "‰Ω†ÊòØ‰∏Ä‰Ωç FDA Ê≥ïË¶èÂêàË¶èËàáÁ≠ñÁï•ÂàÜÊûêÂ∞àÂÆ∂ÔºåË´ã‰ΩøÁî®ÁπÅÈ´î‰∏≠ÊñáÂõûÁ≠î„ÄÇ",

        )



    return selected_agent





# =========================

#  API Keys

# =========================



def get_env_or_session_api_key(session_key: str, env_var: str) -> str:

    if session_key in st.session_state and st.session_state[session_key]:

        return st.session_state[session_key]



    env_val = os.getenv(env_var)

    if env_val:

        # Use environment key silently

        st.session_state[session_key] = env_val

        return env_val

    return ""





def render_api_key_inputs():

    st.sidebar.markdown(f"### üîê {t('api_key_section')}")

    with st.sidebar.expander("OpenAI", expanded=False):

        env_val = os.getenv("OPENAI_API_KEY")

        if env_val:

            st.markdown("Using environment OpenAI API keyÔºà‰∏çÈ°ØÁ§∫ÂØ¶ÈöõÂÄºÔºâ„ÄÇ")

            st.session_state["openai_api_key"] = env_val

        else:

            st.session_state["openai_api_key"] = st.text_input(

                "OpenAI API Key",

                type="password",

                value=st.session_state.get("openai_api_key", ""),

            )



    with st.sidebar.expander("Gemini", expanded=False):

        env_val = os.getenv("GEMINI_API_KEY")

        if env_val:

            st.markdown("Using environment Gemini API keyÔºà‰∏çÈ°ØÁ§∫ÂØ¶ÈöõÂÄºÔºâ„ÄÇ")

            st.session_state["gemini_api_key"] = env_val

        else:

            st.session_state["gemini_api_key"] = st.text_input(

                "Gemini API Key",

                type="password",

                value=st.session_state.get("gemini_api_key", ""),

            )



    with st.sidebar.expander("Anthropic", expanded=False):

        env_val = os.getenv("ANTHROPIC_API_KEY")

        if env_val:

            st.markdown("Using environment Anthropic API keyÔºà‰∏çÈ°ØÁ§∫ÂØ¶ÈöõÂÄºÔºâ„ÄÇ")

            st.session_state["anthropic_api_key"] = env_val

        else:

            st.session_state["anthropic_api_key"] = st.text_input(

                "Anthropic API Key",

                type="password",

                value=st.session_state.get("anthropic_api_key", ""),

            )



    with st.sidebar.expander("XAI (Grok)", expanded=False):

        env_val = os.getenv("XAI_API_KEY")

        if env_val:

            st.markdown("Using environment XAI API keyÔºà‰∏çÈ°ØÁ§∫ÂØ¶ÈöõÂÄºÔºâ„ÄÇ")

            st.session_state["xai_api_key"] = env_val

        else:

            st.session_state["xai_api_key"] = st.text_input(

                "XAI API Key",

                type="password",

                value=st.session_state.get("xai_api_key", ""),

            )





# =========================

#  Model & Prompt Controls

# =========================



MODEL_CATALOG = {

    "OpenAI": [

        {"id": "gpt-4o-mini", "label": "GPT‚Äë4o mini"},

        {"id": "gpt-4.1-mini", "label": "GPT‚Äë4.1 mini"},

    ],

    "Gemini": [

        {"id": "gemini-2.5-flash", "label": "Gemini 2.5 Flash"},

        {"id": "gemini-3-flash", "label": "Gemini 3 Flash"},

    ],

    "Anthropic": [

        {"id": "claude-3.5-sonnet", "label": "Claude 3.5 Sonnet"},

        {"id": "claude-3.5-haiku", "label": "Claude 3.5 Haiku"},

    ],

    "XAI (Grok)": [

        {"id": "grok-4", "label": "Grok-4 (XAI)"},

    ],

}





def render_llm_controls():

    st.sidebar.markdown("### üß† LLM & Prompt")

    provider = st.sidebar.selectbox(

        t("provider"),

        list(MODEL_CATALOG.keys()),

        index=list(MODEL_CATALOG.keys()).index(st.session_state.get("llm_provider", "Gemini")),

        key="llm_provider",

    )

    models = MODEL_CATALOG[provider]

    model_ids = [m["id"] for m in models]



    default_model = st.session_state.get("llm_model_id", model_ids[0])

    if default_model not in model_ids:

        default_model = model_ids[0]



    model_id = st.sidebar.selectbox(

        t("model"),

        options=model_ids,

        index=model_ids.index(default_model),

        format_func=lambda m: next(x["label"] for x in models if x["id"] == m),

        key="llm_model_id",

    )



    max_tokens = st.sidebar.slider(

        t("max_tokens"), min_value=256, max_value=8192, value=int(st.session_state.get("llm_max_tokens", 4096)), step=256,

        key="llm_max_tokens",

    )

    temperature = st.sidebar.slider(

        t("temperature"),

        min_value=0.0,

        max_value=1.5,

        value=float(st.session_state.get("llm_temperature", 0.3)),

        step=0.05,

        key="llm_temperature",

    )

    system_prompt = st.sidebar.text_area(

        t("custom_prompt"),

        value=st.session_state.get("llm_system_prompt", ""),

        key="llm_system_prompt",

        height=180,

    )

    return provider, model_id, max_tokens, temperature, system_prompt





def get_llm_config():

    return (

        st.session_state.get("llm_provider", "Gemini"),

        st.session_state.get("llm_model_id", "gemini-3-flash"),

        int(st.session_state.get("llm_max_tokens", 4096)),

        float(st.session_state.get("llm_temperature", 0.3)),

        st.session_state.get("llm_system_prompt", "‰Ω†ÊòØ‰∏Ä‰Ωç FDA Ê≥ïË¶èÂêàË¶èËàáÁ≠ñÁï•ÂàÜÊûêÂ∞àÂÆ∂ÔºåË´ã‰ΩøÁî®ÁπÅÈ´î‰∏≠ÊñáÂõûÁ≠î„ÄÇ"),

    )





# =========================

#  LLM Call Wrapper

# =========================



def call_llm(

    provider: str,

    model: str,

    system_prompt: str,

    user_messages: List[Dict[str, str]],

    max_tokens: int = 2048,

    temperature: float = 0.4,

) -> str:

    if provider == "OpenAI":

        api_key = st.session_state.get("openai_api_key") or os.getenv("OPENAI_API_KEY")

        if not api_key:

            st.error("OpenAI API key is required.")

            return ""

        client = OpenAI(api_key=api_key)

        messages = [{"role": "system", "content": system_prompt}] + user_messages

        resp = client.chat.completions.create(

            model=model,

            messages=messages,

            max_tokens=max_tokens,

            temperature=temperature,

        )

        return resp.choices[0].message.content



    elif provider == "Gemini":

        api_key = st.session_state.get("gemini_api_key") or os.getenv("GEMINI_API_KEY")

        if not api_key:

            st.error("Gemini API key is required.")

            return ""

        genai.configure(api_key=api_key)

        model_obj = genai.GenerativeModel(model)

        full_prompt = f"{system_prompt}\n\n" + "\n\n".join(

            f"{m['role'].upper()}: {m['content']}" for m in user_messages

        )

        resp = model_obj.generate_content(

            full_prompt,

            generation_config=genai.types.GenerationConfig(

                max_output_tokens=max_tokens,

                temperature=temperature,

            ),

        )

        return resp.text



    elif provider == "Anthropic":

        api_key = st.session_state.get("anthropic_api_key") or os.getenv("ANTHROPIC_API_KEY")

        if not api_key:

            st.error("Anthropic API key is required.")

            return ""

        client = anthropic.Anthropic(api_key=api_key)

        messages = [m for m in user_messages if m["role"] != "system"]

        resp = client.messages.create(

            model=model,

            max_tokens=max_tokens,

            temperature=temperature,

            system=system_prompt,

            messages=[{"role": m["role"], "content": m["content"]} for m in messages],

        )

        return "".join(block.text for block in resp.content if hasattr(block, "text"))



    elif provider == "XAI (Grok)":

        # Sample XAI usage, following your reference

        api_key = st.session_state.get("xai_api_key") or os.getenv("XAI_API_KEY")

        if not api_key:

            st.error("XAI API key is required.")

            return ""

        client = XAIClient(api_key=api_key, timeout=3600)

        chat = client.chat.create(model=model)

        chat.append(xai_system(system_prompt))

        for m in user_messages:

            if m["role"] == "user":

                chat.append(xai_user(m["content"]))

        response = chat.sample()

        return response.content



    else:

        st.error("Unsupported provider.")

        return ""





# =========================

#  File Utilities

# =========================



def extract_text_from_pdf(file_bytes: BytesIO) -> str:

    reader = PdfReader(file_bytes)

    texts = []

    for page in reader.pages:

        texts.append(page.extract_text() or "")

    return "\n".join(texts)





def extract_text_from_docx(file_bytes: BytesIO) -> str:

    return docx2txt.process(file_bytes)





def extract_text_from_txt(file_bytes: BytesIO) -> str:

    return file_bytes.read().decode("utf-8", errors="ignore")





def extract_text(uploaded_file) -> str:

    name = uploaded_file.name.lower()

    data = BytesIO(uploaded_file.read())

    if name.endswith(".pdf"):

        return extract_text_from_pdf(data)

    elif name.endswith(".docx"):

        return extract_text_from_docx(data)

    elif name.endswith(".txt"):

        return extract_text_from_txt(data)

    elif name.endswith(".md"):

        return data.read().decode("utf-8", errors="ignore")

    else:

        st.error("Unsupported format. Please upload PDF, DOCX, TXT, or MD.")

        return ""





def markdown_to_pdf_bytes(md_text: str) -> bytes:

    pdf = FPDF()

    pdf.set_auto_page_break(auto=True, margin=15)

    pdf.add_page()

    pdf.set_font("Arial", size=11)

    for line in md_text.splitlines():

        pdf.multi_cell(0, 5, line)

    pdf_bytes = BytesIO()

    pdf.output(pdf_bytes)

    pdf_bytes.seek(0)

    return pdf_bytes.getvalue()





# =========================

#  Prompts

# =========================



def build_deep_summary_prompt(doc_text: str, lang: str) -> str:

    if lang == "en":

        language_instruction = "Write the entire output in English."

    else:

        language_instruction = "Ë´ã‰ΩøÁî®ÁπÅÈ´î‰∏≠ÊñáÊí∞ÂØ´Êï¥‰ªΩËº∏Âá∫Ôºå‰∏¶‰ª• FDA ÂØ©Êü•ËàáÂêàË¶èË¶ñËßíÈÄ≤Ë°åÊ∑±ÂÖ•ÂàÜÊûê„ÄÇ"



    base = f"""

‰Ω†ÊòØ‰∏Ä‰ΩçÂÖ∑ÂÇô FDA Ë¶èÁØÑ„ÄÅÈÜ´Ëó•/ÈÜ´ÊùêÂØ©Êü•ËàáÊà∞Áï•Ë¶èÂäÉÂ∞àÈï∑ÁöÑ„ÄåÈ´òÈöéÁ≠ñÁï•ÂØ©Èñ±ÂÆò„ÄçËàá„ÄåÁü•Ë≠òÊû∂ÊßãÂ∏´„Äç„ÄÇ

{language_instruction}



‰Ω†Â∞áÊî∂Âà∞‰∏Ä‰ªΩÊñá‰ª∂ÂÖßÂÆπÔºàÂèØËÉΩÁÇ∫Ëó•ÂìÅ„ÄÅÈÜ´ÁôÇÂô®Êùê„ÄÅÁîüÈÜ´Áµ±Ë®à„ÄÅËá®Â∫äË©¶È©ó„ÄÅÂìÅË≥™Á≥ªÁµ±„ÄÅÈ¢®Èö™ÁÆ°ÁêÜÊàñÂÖ∂‰ªñËàá FDA Áõ∏Èóú‰πãÂÖßÂÆπÔºâ„ÄÇ

Ë´ãÂü∑Ë°å‰ª•‰∏ã‰ªªÂãôÔºö



1. Áî¢Âá∫‰∏Ä‰ªΩ **Ê∑±Â∫¶„ÄÅÁµêÊßãÂåñÁöÑ Markdown Â†±Âëä**ÔºåÈï∑Â∫¶Á¥Ñ **2,000‚Äì3,000 Â≠ó**„ÄÇ

2. Â†±ÂëäÈúÄÂêåÊôÇÈóúÊ≥®ÔºöÂÖßÂÆπÊú¨Ë∫´ÁöÑÈÇèËºØ„ÄÅFDA ÂêàË¶èË¶ÅÈªû„ÄÅÊΩõÂú®È¢®Èö™ËàáÁº∫Âè£„ÄÇ

3. ‰ΩøÁî®‰ª•‰∏ãÁµêÊßãÔºàMarkdown Ê®ôÈ°åÔºâÔºö



# Executive Overview / Êñá‰ª∂Á∏ΩË¶Ω

- Êñá‰ª∂ÁõÆÁöÑ„ÄÅÁõÆÊ®ôÂ∞çË±°ËàáÊ†∏ÂøÉ‰∏ªÈ°å„ÄÇ



## Key Themes & Regulatory Objectives / ÈóúÈçµ‰∏ªÈ°åËàáÊ≥ïË¶èÁõÆÊ®ô

- Ê¢ùÂàóÊñá‰ª∂Ê¨≤ÈÅîÊàê‰πã FDA ÂêàË¶èÊàñ‰∏äÂ∏ÇÁ≠ñÁï•ÁõÆÊ®ô„ÄÇ



## Section-by-Section Analysis / ÈÄêÊÆµÊ∑±Â∫¶Ëß£Êûê

- ÈáùÂ∞çÈáçË¶ÅÊÆµËêΩÊàñÁ´†ÁØÄÔºåË™™ÊòéÔºö

  - ‰∏ªË¶ÅÂÖßÂÆπÂú®Ë´á‰ªÄÈ∫ºÔºü

  - Ëàá FDA Ê≥ïË¶è„ÄÅÂØ©Êü•ËßÄÈªûÁöÑÈóúËÅØÔºü

  - ÊΩõÂú®È¢®Èö™„ÄÅÁñëÈªûÊàñÈúÄË¶ÅË£úÂº∑‰πãËôïÔºü



## Critical Risks, Gaps, Red Flags / ÈóúÈçµÈ¢®Èö™ËàáÁº∫Âè£

- ÂæûÊ≥ïË¶è„ÄÅËá®Â∫ä„ÄÅCMC„ÄÅÁµ±Ë®à„ÄÅÂÆâÂÖ®ÊÄß„ÄÅÊ®ôÁ§∫ËàáË™™ÊòéÊõ∏Á≠âÂ§öÈù¢ÂêëÔºåÈªûÂá∫Ôºö

  - È´òÔºè‰∏≠Ôºè‰ΩéÈ¢®Èö™È†ÖÁõÆ

  - ÂèØËÉΩÈÅ≠ FDA Ë≥™ÁñëÊàñË¶ÅÊ±ÇË£ú‰ª∂‰πãËôï„ÄÇ



## Actionable Recommendations / ÂèØÂü∑Ë°åÂª∫Ë≠∞

- Ê¢ùÂàóÂÖ∑È´î„ÄÅÂèØÊìç‰Ωú‰πã‰∏ã‰∏ÄÊ≠•Ôºö

  - ‰æãÂ¶ÇÈúÄË£úÂÖÖÂì™‰∫õË©¶È©ó„ÄÅË£úÂº∑Âì™‰∫õÊ®°ÁµÑ„ÄÅÂ¢ûÂä†Âì™‰∫õÈ¢®Èö™ÊéßÁÆ°Ë™™ÊòéÁ≠â„ÄÇ



## Stakeholder-Specific Views / Âà©ÂÆ≥Èóú‰øÇ‰∫∫Ë¶ñËßí

- Ë™™ÊòéÂ∞ç‰ª•‰∏ãËßíËâ≤ÁöÑÈáçË¶ÅËß£ËÆÄÔºö

  - Ê≥ïË¶è‰∫ãÂãôÔºàRAÔºâ

  - Ëá®Â∫äËàáÈÜ´Â≠∏ÂúòÈöä

  - Áµ±Ë®àËàáÊï∏ÊìöÁßëÂ≠∏

  - ÂìÅË≥™ËàáËó•Âª†ÔºèÂ∑•Âª†ÁáüÈÅã

  - ÁÆ°ÁêÜÈöéÂ±§ÔºèÊ±∫Á≠ñËÄÖ



## Glossary of Key Terms (if applicable) / Â∞àÊúâÂêçË©ûÊï¥ÁêÜ

- Â∞áÈóúÈçµ FDAÔºèÊäÄË°ìË°ìË™ûÊ¢ùÂàó‰∏¶ÂÅöÁ∞°ÊòéÂÆöÁæ©„ÄÇ



ÈôêÂà∂Ê¢ù‰ª∂Ôºö

- ‰ΩøÁî® Markdown Ê®ôÈ°åÔºà#„ÄÅ##„ÄÅ###ÔºâËàáÊ¢ùÂàó„ÄÇ

- ÂÑ™ÂÖàÈÅøÂÖçËôõÊßãÁâπÂÆöÊï∏ÊìöÔºõÂ¶ÇÊñá‰ª∂Êú™Êèê‰æõÔºåË´ã‰ª•„ÄåÊñá‰ª∂Êú™ÊòéÁ¢∫Ë™™Êòé„ÄçÊ®ôÁ§∫„ÄÇ

- Ëã•Êñá‰ª∂ÂÖßÂÆπ‰∏çË∂≥‰ª•ÊîØÊíêÁµêË´ñÔºåÈúÄÂú®Êñá‰∏≠Ê∏ÖÊ•öË®ªÊòé‰∏çÁ¢∫ÂÆöÊÄß„ÄÇ



‰ª•‰∏ãÁÇ∫Êñá‰ª∂ÂÖßÂÆπÔºàÂèØËÉΩÂ∑≤ÁÇ∫Èï∑Â∫¶ËÄÉÈáèËÄåÊà™Êñ∑ÔºâÔºö



[DOCUMENT START]

{doc_text[:100000]}

[DOCUMENT END]

"""

    return base.strip()





# =========================

#  Tabs

# =========================



def tab_file_transform_deep_summary():

    st.markdown(f"## {t('tab_file_transform')}")

    st.markdown('<div class="glass-panel">', unsafe_allow_html=True)



    uploaded = st.file_uploader(

        t("upload_label"),

        type=["pdf", "docx", "txt"],

        key="file_transform_uploader",

    )



    output_format = st.radio(

        t("output_format"),

        [t("format_markdown"), t("format_pdf")],

        horizontal=True,

        key="output_format_choice",

    )



    if uploaded is not None:

        if st.button(t("run_summary"), type="primary"):

            with st.spinner("Extracting text and generating deep summary‚Ä¶"):

                raw_text = extract_text(uploaded)

                if not raw_text.strip():

                    st.error("No readable text extracted from the file.")

                    st.markdown("</div>", unsafe_allow_html=True)

                    return



                provider, model_id, max_tokens, temperature, system_prompt = get_llm_config()

                lang = st.session_state.get("ui_lang", "zh")



                prompt = build_deep_summary_prompt(raw_text, lang)

                output = call_llm(

                    provider=provider,

                    model=model_id,

                    system_prompt=system_prompt,

                    user_messages=[{"role": "user", "content": prompt}],

                    max_tokens=max_tokens,

                    temperature=temperature,

                )

                if not output:

                    st.markdown("</div>", unsafe_allow_html=True)

                    return



                st.session_state["latest_file_text"] = raw_text

                st.session_state["latest_file_summary_md"] = output

                st.session_state["latest_file_name"] = uploaded.name



                st.markdown("### üìÑ Deep Summary (Markdown)")

                st.markdown(output)



                if output_format == t("format_markdown"):

                    st.download_button(

                        "Download Markdown",

                        data=output.encode("utf-8"),

                        file_name=f"{uploaded.name}.summary.md",

                        mime="text/markdown",

                    )

                else:

                    pdf_bytes = markdown_to_pdf_bytes(output)

                    st.download_button(

                        "Download PDF",

                        data=pdf_bytes,

                        file_name=f"{uploaded.name}.summary.pdf",

                        mime="application/pdf",

                    )



    if "latest_file_text" in st.session_state:

        st.markdown("---")

        st.markdown(f"### üí¨ {t('chat_with_file')} ‚Äî {st.session_state.get('latest_file_name', '')}")

        user_q = st.text_area(t("user_prompt"), key="file_chat_prompt")

        if st.button("Ask the file"):

            provider, model_id, max_tokens, temperature, system_prompt = get_llm_config()

            full_context = f"""

‰ª•‰∏ãÊòØÂéüÂßãÊñá‰ª∂ÂÖßÂÆπËàáË©≤Êñá‰ª∂‰πãÈï∑ÁØáÊëòË¶Å„ÄÇË´ãÂö¥Ê†ºÊ†πÊìöÊ≠§Á≠âË≥áË®ä‰ΩúÁ≠îÔºåËã•ÂÖßÂÆπ‰∏çË∂≥‰ª•ÊîØÊåÅÁ≠îÊ°àÔºåË´ãÊòéÁ¢∫Ë™™Êòé„ÄåÊñá‰ª∂Êú™Êèê‰æõË∂≥Â§†Ë≥áË®ä„Äç„ÄÇ



[ORIGINAL DOCUMENT]

{st.session_state['latest_file_text'][:60000]}



[SUMMARY]

{st.session_state['latest_file_summary_md'][:40000]}

"""

            question = user_q.strip()

            if not question:

                st.warning("Ë´ãËº∏ÂÖ•ÂïèÈ°å„ÄÇ")

            else:

                with st.spinner("Thinking with the document‚Ä¶"):

                    answer = call_llm(

                        provider=provider,

                        model=model_id,

                        system_prompt=system_prompt,

                        user_messages=[

                            {"role": "user", "content": full_context},

                            {"role": "user", "content": question},

                        ],

                        max_tokens=max_tokens,

                        temperature=temperature,

                    )

                st.markdown("#### Answer")

                st.markdown(answer or "_No answer produced._")



    st.markdown("</div>", unsafe_allow_html=True)





def tab_file_intelligence():

    st.markdown(f"## {t('tab_file_intel')}")

    st.markdown('<div class="glass-panel">', unsafe_allow_html=True)

    up = st.file_uploader(

        t("upload_label"),

        type=["pdf", "docx", "txt", "md"],

        key="file_intel_uploader",

    )

    if up is not None and st.button("Analyze File"):

        with st.spinner("Analyzing file‚Ä¶"):

            text = extract_text(up)

            provider, model_id, max_tokens, temperature, system_prompt = get_llm_config()

            lang = st.session_state.get("ui_lang", "zh")



            language_instruction = (

                "Write the output in English."

                if lang == "en"

                else "Ë´ã‰ΩøÁî®ÁπÅÈ´î‰∏≠ÊñáÊí∞ÂØ´Ôºå‰∏¶‰ª• FDA ÂØ©Êü•ËàáÂêàË¶èËßÄÈªûÈÄ≤Ë°åË™™Êòé„ÄÇ"

            )

            prompt = f"""

‰Ω†ÊòØ‰∏Ä‰Ωç FDA Ê≥ïË¶è„ÄÅËá®Â∫äËàá CMC Êï¥ÂêàÂàÜÊûêÂ∞àÂÆ∂„ÄÇ

{language_instruction}



Ë´ãÈáùÂ∞ç‰ª•‰∏ãÊñá‰ª∂ÈÄ≤Ë°åÁµêÊßãÂåñÂàÜÊûêÔºåÊ∂µËìãÔºö

- Êñá‰ª∂ÁõÆÁöÑËàáÈÅ©Áî®È†òÂüü

- Ëàá FDA Áõ∏ÈóúÁöÑÊ≥ïË¶èÊàñÊåáÂºïÔºàÂ¶Ç 21 CFR„ÄÅGxP„ÄÅICH ÊåáÂçóÔºâ‰πãÈóúËÅØ

- ÊΩõÂú®È¢®Èö™ËàáÁº∫Âè£

- Âª∫Ë≠∞Ë£úÂº∑Ëàá‰∏ã‰∏ÄÊ≠•Ë°åÂãï



[DOCUMENT START]

{text[:100000]}

[DOCUMENT END]

"""

            result = call_llm(

                provider=provider,

                model=model_id,

                system_prompt=system_prompt,

                user_messages=[{"role": "user", "content": prompt}],

                max_tokens=max_tokens,

                temperature=temperature,

            )

            st.markdown("### Analysis")

            st.markdown(result or "_No output._")

    st.markdown("</div>", unsafe_allow_html=True)





def tab_multi_file_synthesis():

    st.markdown(f"## {t('tab_multi_file')}")

    st.markdown('<div class="glass-panel">', unsafe_allow_html=True)

    files = st.file_uploader(

        "Upload multiple files (PDF/DOCX/TXT/MD)",

        type=["pdf", "docx", "txt", "md"],

        accept_multiple_files=True,

        key="multi_files",

    )

    if files and st.button("Combine & Analyze"):

        with st.spinner("Combining and analyzing files‚Ä¶"):

            assembled = []

            for f in files:

                content = extract_text(f)

                assembled.append(

                    f"--- START FILE: {f.name} ---\n{content}\n--- END FILE: {f.name} ---\n"

                )

            combined = "\n".join(assembled)[:150000]



            provider, model_id, max_tokens, temperature, system_prompt = get_llm_config()

            lang = st.session_state.get("ui_lang", "zh")

            language_instruction = (

                "Write the output in English."

                if lang == "en"

                else "Ë´ã‰ΩøÁî®ÁπÅÈ´î‰∏≠ÊñáÊí∞ÂØ´Ôºå‰∏¶Âº∑Ë™øË∑®Êñá‰ª∂‰πã FDA Ê≥ïË¶èËßÄÈªûËàáÂ∑ÆÁï∞„ÄÇ"

            )



            prompt = f"""

‰Ω†ÊòØ‰∏Ä‰ΩçÂ∞àÁ≤æÊñº FDA Â†±Ë¶èËàáË∑®Êñá‰ª∂Á≠ñÁï•Ë©ï‰º∞ÁöÑÈ°ßÂïè„ÄÇ



{language_instruction}



‰Ω†Â∞áÊî∂Âà∞Â§ö‰ªΩÊñá‰ª∂ÔºåÂ∑≤‰ª• START/END FILE Ê®ôË®òÂçÄÂàÜ„ÄÇ

Ë´ãË¶ñÂÖ∂ÁÇ∫‰∏ÄÁµÑ„ÄåÁü•Ë≠òÂ∫´„ÄçÔºåÂü∑Ë°å‰ª•‰∏ã‰ªªÂãôÔºö



- ÊØîËºÉËàáÂ∞çÁÖßÂêÑÊñá‰ª∂Âú®Ê≥ïË¶èÁ´ãÂ†¥„ÄÅËá®Â∫äË≠âÊìö„ÄÅCMC„ÄÅÈ¢®Èö™ÁÆ°ÁêÜÁ≠âÈù¢ÂêëÁöÑÂ∑ÆÁï∞Ëàá‰∏ÄËá¥ÊÄß„ÄÇ

- ÊâæÂá∫ÈóúÈçµËêΩÂ∑ÆÔºà‰æãÂ¶Ç CTD Ê®°ÁµÑÈñìÂâçÂæå‰∏ç‰∏ÄËá¥„ÄÅÁµ±Ë®àÂÅáË®≠ËàáÂØ¶ÈöõÂàÜÊûê‰∏çÁ¨¶„ÄÅË™™ÊòéÊõ∏ËàáÊ®ôÁ§∫‰∏ç‰∏ÄËá¥Á≠âÔºâ„ÄÇ

- Áî¢Âá∫ Markdown Â†±ÂëäÔºåÂåÖÂê´Ôºö

  - Executive SummaryÔºàÊï¥È´îÁµêË´ñÔºâ

  - Cross-Document ComparisonsÔºàË∑®Êñá‰ª∂ÊØîËºÉÔºâ

  - Key Risks / GapsÔºàÈ¢®Èö™ËàáÁº∫Âè£Ôºâ

  - FDA ÂØ©Êü•ËßÄÈªû‰∏ãÁöÑÂÑ™ÂÖàÈ†ÜÂ∫èËàáÂª∫Ë≠∞‰∏ã‰∏ÄÊ≠•



[DOCUMENTS]

{combined}

"""

            result = call_llm(

                provider=provider,

                model=model_id,

                system_prompt=system_prompt,

                user_messages=[{"role": "user", "content": prompt}],

                max_tokens=max_tokens,

                temperature=temperature,

            )

            st.markdown("### Synthesis Report")

            st.markdown(result or "_No output._")

    st.markdown("</div>", unsafe_allow_html=True)





def tab_smart_replace():

    st.markdown(f"## {t('tab_smart_replace')}")

    st.markdown('<div class="glass-panel">', unsafe_allow_html=True)

    col1, col2 = st.columns(2)

    with col1:

        template_text = st.text_area(

            "Template (with placeholders like [Product Name], [Indication])",

            height=260,

        )

    with col2:

        context_text = st.text_area(

            "Context / Raw Data Source (e.g., protocol, CSR, CMC summary)",

            height=260,

        )



    instructions = st.text_area(

        "Natural language instructions (tone, style, constraints)",

        value="Ë´ã‰æùÁÖß FDA Ê≥ïË¶èËàáÁßëÂ≠∏ÂêàÁêÜÊÄßÂ°´ÂØ´ÊâÄÊúâÊ¨Ñ‰ΩçÔºåÁ∂≠ÊåÅÂ∞àÊ•≠„ÄÅÁ≤æÁ¢∫‰∏îÂØ©Êü•ÂèãÂñÑÁöÑË™ûÊ∞£„ÄÇ",

    )



    if st.button("Run Smart Replace"):

        provider, model_id, max_tokens, temperature, system_prompt = get_llm_config()

        lang = st.session_state.get("ui_lang", "zh")

        language_instruction = (

            "Write the output in English."

            if lang == "en"

            else "Ë´ã‰ΩøÁî®ÁπÅÈ´î‰∏≠ÊñáÊí∞ÂØ´ÂÆåÊï¥ÁØÑÊú¨ÂÖßÂÆπ„ÄÇ"

        )



        prompt = f"""

‰Ω†ÊòØ‰∏Ä‰Ωç FDA Â†±Ë¶èËàáÊ≥ïÂæãÊñáÊú¨Êí∞ÂØ´Â∞àÂÆ∂„ÄÇ



{language_instruction}



‰∏ãÂàóÁÇ∫‰∏Ä‰ªΩÂê´ÊúâÂç†‰ΩçÁ¨¶ÁöÑÁØÑÊú¨ÔºàÂ¶Ç [Product Name]„ÄÅ[Indication]„ÄÅ[Dosage] Á≠âÔºâÔºö



[TEMPLATE]

{template_text}



‰ª•‰∏ãÁÇ∫Êú™ÁµêÊßãÂåñÁöÑËÉåÊôØË≥áÊñôÔºàÂèØËÉΩ‰æÜËá™ ICH CTD Ê®°ÁµÑ„ÄÅËá®Â∫äË©¶È©óË®àÁï´„ÄÅCMC Êñá‰ª∂„ÄÅÈ¢®Èö™ÁÆ°ÁêÜË®àÁï´Á≠âÔºâÔºö

[CONTEXT]

{context_text}



‰ΩøÁî®ËÄÖÁµ¶‰Ω†ÁöÑÈ°çÂ§ñË™™ÊòéËàáÂÅèÂ•ΩÔºàË™ûÊ∞£„ÄÅÈ¢®Ê†º„ÄÅÈôêÂà∂ÔºâÂ¶Ç‰∏ãÔºö

{instructions}



Ë´ã‰æùÊìö CONTEXT ‰∏≠ÂèØÂêàÁêÜÊé®Ë´ñ‰πãË≥áË®äÔºö

- Ë£úÈΩäÊâÄÊúâÂç†‰ΩçÁ¨¶

- ÈÅøÂÖçÊÜëÁ©∫ÊçèÈÄ†ÈóúÈçµÊï∏ÊìöÔºõËã•Êñá‰ª∂Êú™Êèê‰æõÔºåË´ã‰ª•„ÄåÔºàÊñá‰ª∂Êú™Êèê‰æõÊòéÁ¢∫Ë≥áË®äÔºâ„ÄçÊ®ôÁ§∫

- Ë™øÊï¥Âë®ÈÇäÊñáÂ≠óÔºå‰ΩøÂÖ®ÊñáÂú®Ë™ûÊ≥ïËàáÊ≥ïË¶èË™ûÊ∞£‰∏äËá™ÁÑ∂„ÄÅÈÄ£Ë≤´

- ‰ª• Markdown Ëº∏Âá∫ÂÆåÊï¥‰∏îÂ∑≤Â°´ÂØ´ÂÆåÊàê‰πãÁØÑÊú¨

"""

        with st.spinner("Generating filled template‚Ä¶"):

            result = call_llm(

                provider=provider,

                model=model_id,

                system_prompt=system_prompt,

                user_messages=[{"role": "user", "content": prompt}],

                max_tokens=max_tokens,

                temperature=temperature,

            )

        st.markdown("### Completed Template")

        st.markdown(result or "_No output._")

    st.markdown("</div>", unsafe_allow_html=True)





def tab_ai_note_keeper():

    st.markdown(f"## {t('tab_note_keeper')}")

    st.markdown('<div class="glass-panel">', unsafe_allow_html=True)



    raw_note = st.text_area("Your raw notes / brain dump", height=240, key="note_raw")

    col1, col2, col3, col4, col5 = st.columns(5)

    action = None

    if col1.button("Format"):

        action = "format"

    if col2.button("Tasks"):

        action = "tasks"

    if col3.button("Fix"):

        action = "fix"

    if col4.button("Summary"):

        action = "summary"

    if col5.button("Expand"):

        action = "expand"



    if action and raw_note.strip():

        provider, model_id, max_tokens, temperature, system_prompt = get_llm_config()

        lang = st.session_state.get("ui_lang", "zh")

        language_instruction = (

            "Write the output in English."

            if lang == "en"

            else "Ë´ã‰ΩøÁî®ÁπÅÈ´î‰∏≠ÊñáÊí∞ÂØ´Ôºå‰∏¶Á∂≠ÊåÅ FDA Â†±Ë¶èÊàñÂ∞àÊ•≠ÂØ©Êü•Êñá‰ª∂Â∏∏Ë¶ã‰πãË™ûÊ∞£„ÄÇ"

        )



        prompt_map = {

            "format": "Â∞áÈÄô‰∫õÁ≠ÜË®òÊï¥ÁêÜÊàêÁµêÊßãÊ∏ÖÊ•öÁöÑ MarkdownÔºàÂê´Ê®ôÈ°åËàáÊ¢ùÂàóÔºâÔºåÊñπ‰æøÊó•ÂæåÁî®Êñº FDA Êñá‰ª∂ËçâÊì¨„ÄÇ",

            "tasks": "ÂæûÈÄô‰∫õÂÖßÂÆπ‰∏≠ËêÉÂèñÊâÄÊúâÂèØÂü∑Ë°å‰ªªÂãôÔºå‰∏¶‰ª•Ê†∏ÂèñÊ∏ÖÂñÆ (- [ ]) Ê¢ùÂàóÔºåËëóÈáçÊñº FDA Â†±Ë¶èËàáÂêàË¶èË°åÂãï„ÄÇ",

            "fix": "‰øÆÊ≠£ÊñáÊ≥ï„ÄÅÁî®Ë©ûËàáÈÇèËºØÔºå‰ΩøÂÖ∂Êõ¥ÈÅ©Âêà‰ΩúÁÇ∫Â∞ç FDA ÊàñÂÖßÈÉ®ÂØ©Êü•‰ΩøÁî®ÁöÑÂ∞àÊ•≠ÊñáÂ≠ó„ÄÇ",

            "summary": "ÂÖàÁµ¶Âá∫‰∏ÄÊÆµÁ≤æÁ∞° TL;DR ÊëòË¶ÅÔºåÂÜç‰ª•Ê¢ùÂàóÊñπÂºèÊï¥ÁêÜÈáçÈªûËàáÈ¢®Èö™È†ÖÁõÆ„ÄÇ",

            "expand": "Â∞áÁ∞°Áü≠ÁöÑË¶ÅÈªûÊì¥ÂØ´ÊàêËºÉÂÆåÊï¥ÁöÑÊÆµËêΩÔºå‰∏¶Âä†ÂÖ• FDA ÂêàË¶èËßÄÈªûÊàñÂØ¶ÂãôÂª∫Ë≠∞„ÄÇ",

        }

        prompt = f"""

‰Ω†ÊòØ‰∏Ä‰ΩçÂ∞àÈñÄÂçîÂä© FDA Â†±Ë¶èÂúòÈöäÊï¥ÁêÜÊÄùË∑ØÁöÑ„ÄåÁü•Ë≠òÁÆ°ÁêÜÈ°ßÂïè„Äç„ÄÇ



{language_instruction}



‰ΩøÁî®ËÄÖÁöÑÂéüÂßãÁ≠ÜË®òÂ¶Ç‰∏ãÔºö

{raw_note}



‰ªªÂãôÔºö{prompt_map[action]}



Ë´ãÂè™Ëº∏Âá∫Êï¥ÁêÜÂæåÁöÑ Markdown Á≠ÜË®ò„ÄÇ

"""

        with st.spinner("Transforming notes‚Ä¶"):

            result = call_llm(

                provider=provider,

                model=model_id,

                system_prompt=system_prompt,

                user_messages=[{"role": "user", "content": prompt}],

                max_tokens=max_tokens,

                temperature=temperature,

            )

        st.markdown("### Transformed Notes")

        st.markdown(result or "_No output._")

    st.markdown("</div>", unsafe_allow_html=True)





# =========================

#  Main

# =========================



def main():

    st.set_page_config(

        page_title="AuditFlow AI ¬∑ Masterpiece Edition (FDA)",

        layout="wide",

    )



    # Init session defaults

    if "ui_lang" not in st.session_state:

        st.session_state.ui_lang = "zh"

    if "dark_mode" not in st.session_state:

        st.session_state.dark_mode = True

    if "artist_style_key" not in st.session_state:

        st.session_state.artist_style_key = "van_gogh"



    # Load agents

    agents = load_agents()



    # Sidebar global controls

    with st.sidebar:

        st.markdown("## üåê Global Settings")

        lang_label = st.radio("Language / Ë™ûË®Ä", ["English", "ÁπÅÈ´î‰∏≠Êñá"], key="lang_radio")

        st.session_state.ui_lang = "en" if lang_label == "English" else "zh"



        dark_mode = st.toggle("Dark mode", value=st.session_state.dark_mode, key="dark_mode_toggle")

        st.session_state.dark_mode = dark_mode



        active_style = style_selector_ui()

        render_api_key_inputs()

        selected_agent = agent_selector_ui(agents)

        render_llm_controls()



    # Apply painter theme

    apply_theme(active_style, st.session_state.dark_mode)



    # Header

    st.markdown(f"<h1 class='accent-title'>{t('app_title')}</h1>", unsafe_allow_html=True)

    st.markdown(t("subtitle"))

    if selected_agent:

        st.markdown(

            f"<div class='accent-chip'>ÁõÆÂâç‰ª£ÁêÜ‰∫∫Ôºö{selected_agent.get('display_name_zh','')}</div>",

            unsafe_allow_html=True,

        )



    tab1, tab2, tab3, tab4, tab5 = st.tabs([

        t("tab_file_transform"),

        t("tab_file_intel"),

        t("tab_multi_file"),

        t("tab_smart_replace"),

        t("tab_note_keeper"),

    ])



    with tab1:

        tab_file_transform_deep_summary()

    with tab2:

        tab_file_intelligence()

    with tab3:

        tab_multi_file_synthesis()

    with tab4:

        tab_smart_replace()

    with tab5:

        tab_ai_note_keeper()





if __name__ == "__main__":

    main()
